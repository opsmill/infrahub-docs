---
title: Installing Infrahub
---

import Tabs from '@theme/Tabs';
import TabItem from '@theme/TabItem';
import VideoPlayer from '../../src/components/VideoPlayer';
import ReferenceLink from "../../src/components/Card";
import EnterpriseBadge from "../../src/components/EnterpriseBadge";
import CodeBlock from '@theme/CodeBlock';
import import_deployTf from '!!raw-loader!../../../development/deploy.tf';
import import_infrahubValuesYaml from '!!raw-loader!../../../development/k8s/infrahub-values.yaml';
import import_minioValuesYaml from '!!raw-loader!../../../development/k8s/minio-values.yaml';
import import_neo4jValuesYaml from '!!raw-loader!../../../development/k8s/neo4j-values.yaml';
import import_prefectValuesYaml from '!!raw-loader!../../../development/k8s/prefect-values.yaml';
import import_rabbitmqValuesYaml from '!!raw-loader!../../../development/k8s/rabbitmq-values.yaml';
import import_redisSentinelDeployYaml from '!!raw-loader!../../../development/k8s/redis-sentinel-proxy-deploy.yaml';
import import_redisSentinelSvcYaml from '!!raw-loader!../../../development/k8s/redis-sentinel-proxy-svc.yaml';
import import_redisValuesYaml from '!!raw-loader!../../../development/k8s/redis-values.yaml';
import import_serviceValuesYaml from '!!raw-loader!../../../development/k8s/service-values.yaml';
import import_taskmanagerdbYaml from '!!raw-loader!../../../development/k8s/taskmanagerdb.yaml';

This guide provides step-by-step instructions for installing Infrahub Community and Enterprise editions. The installation methods covered here are for non-resilient deployment architectures suitable for development, testing, and single-node production environments.

For resilient, high-availability deployments, refer to the [high availability architecture](../topics/architecture.mdx#high-availability-deployment) documentation.

## Prerequisites

- Ensure your system meets the [hardware requirements](../topics/hardware-requirements.mdx) before installing Infrahub
- Each installation method has additional prerequisites listed in their respective sections

:::info

Allocating more CPU cores to the Neo4j database will only improve performance on Infrahub Enterprise as it leverages parallel query execution.

:::

<div style={{ textAlign: 'center' }}>
  <VideoPlayer url='https://www.youtube.com/watch?v=onVhaTVFkrM' light />
</div>

## Community

Infrahub Community is deployed as a container-based architecture and can be installed using several methods.

<Tabs>
<TabItem value="Docker compose via curl" default>

### Using curl and Docker Compose

To quickly spin up the latest Infrahub locally, retrieve the Docker Compose file from [infrahub.opsmill.io](https://infrahub.opsmill.io).

You can also specify a specific version or the `develop` branch in the URL:

- [https://infrahub.opsmill.io/develop](https://infrahub.opsmill.io/develop)
- [https://infrahub.opsmill.io/1.3.3](https://infrahub.opsmill.io/1.3.3)

#### Prerequisites

- [Docker](https://docs.docker.com/engine/install/) (version 24.x minimum)
- [Docker Compose](https://docs.docker.com/compose/install/)

#### Start an Infrahub environment

<Tabs groupId="os">
<TabItem value="macOS" default>

```bash
curl https://infrahub.opsmill.io > docker-compose.yml
docker compose -p infrahub up -d
```

</TabItem>
<TabItem value="Linux">

```bash
curl https://infrahub.opsmill.io > docker-compose.yml
sudo docker compose -p infrahub up -d
```

</TabItem>
</Tabs>

After running the command, you should see Docker downloading the necessary images and starting the containers.

:::success

Verify that Infrahub is running by accessing the web interface or checking container status:

```bash
docker ps | grep infrahub
```

:::

#### Stop and remove an Infrahub environment

<Tabs groupId="os">
<TabItem value="macOS" default>

```bash
curl https://infrahub.opsmill.io > docker-compose.yml
docker compose -p infrahub down -v
```

</TabItem>
<TabItem value="Linux">

```bash
curl https://infrahub.opsmill.io > docker-compose.yml
sudo docker compose -p infrahub down -v
```

</TabItem>
</Tabs>

</TabItem>
<TabItem value="Local development (git clone)">

### Cloning the repository

The recommended method for running Infrahub for development uses the Docker Compose files included with the project combined with helper commands defined in `invoke`.

:::info

This method is suitable for local development and demo environments. It is not recommended for production deployments.

:::

#### Prerequisites

- [uv](https://docs.astral.sh/uv/)
- [Docker](https://docs.docker.com/engine/install/) (version 24.x minimum)

#### Step 1: clone the repository

Create the base directory for the Infrahub installation. For this guide, we'll use `/opt/infrahub`.

```bash
cd /opt/
```

:::warning

Usage of the `/opt/infrahub` directory is merely a suggestion. You can use any directory on your system, especially for development or demo purposes.

```bash
mkdir -p ~/source/
cd ~/source/
```

:::

Clone Infrahub repository into the current directory:

```bash
git clone --recursive https://github.com/opsmill/infrahub.git
```

:::success

The `git clone` command should generate output similar to the following:

```bash
Cloning into '.'...
remote: Enumerating objects: 1312, done.
remote: Counting objects: 100% (1312/1312), done.
remote: Compressing objects: 100% (1150/1150), done.
remote: Total 1312 (delta 187), reused 691 (delta 104), pack-reused 0
Receiving objects: 100% (1312/1312), 33.37 MiB | 14.46 MiB/s, done.
Resolving deltas: 100% (187/187), done.
```

:::

#### Step 2: install dependencies

Navigate to the cloned Infrahub directory:

```bash
cd infrahub
```

Install the Python dependencies by running:

```bash
uv sync --all-groups
```

:::success

You should see uv installing the required dependencies. When complete, you'll be returned to the command prompt without errors.

:::

#### Step 3: start Infrahub

Start and initialize Infrahub:

```bash
uv run invoke demo.start
```

<ReferenceLink title="Explore additional Invoke commands for development" url="../topics/local-demo-environment" />

</TabItem>
<TabItem value="Kubernetes with Helm">

### Using Helm and Kubernetes

It's possible to deploy Infrahub on Kubernetes using Helm charts. This method is suitable for production deployments and provides a more resilient architecture.

<ReferenceLink title="Infrahub Helm Chart" url="https://github.com/opsmill/infrahub-helm/tree/stable/charts/infrahub" openInNewTab />
<ReferenceLink title="ArtifactHub" url="https://artifacthub.io/packages/helm/infrahub/infrahub" openInNewTab />

#### Prerequisites

- A Kubernetes cluster
- [Helm](https://helm.sh/docs/intro/install/) installed on your system

#### Production deployment requirements

The following are required for production deployments using Helm:

- Data persistence for the database must be enabled
- Multiple replicas of the Infrahub API Server and Infrahub Task workers should be deployed: you can use the `affinity` variable to define the affinity policy for the pods
- S3 storage should be configured for the Infrahub API Server, it is required if you have multiple replicas

:::warning

We do not recommend using the included dependencies (Neo4j, RabbitMQ, Redis) for production.
They are present to ease deployment on non-production environments.

:::

#### Step 1: Fill in the values file

Create a `values.yml` file with the following configuration:

```yaml
infrahubServer:
  replicas: 3
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: service
              operator: In
              values:
              - infrahub-server
          topologyKey: topology.kubernetes.io/zone
  persistence:
    enabled: false
  ingress:
    enabled: true
  infrahubServer:
    env:
      INFRAHUB_ALLOW_ANONYMOUS_ACCESS: "true"
      INFRAHUB_CACHE_PORT: 6379
      INFRAHUB_DB_TYPE: neo4j
      INFRAHUB_LOG_LEVEL: INFO
      INFRAHUB_PRODUCTION: "true"
      INFRAHUB_INITIAL_ADMIN_TOKEN: 06438eb2-8019-4776-878c-0941b1f1d1ec
      INFRAHUB_SECURITY_SECRET_KEY: 327f747f-efac-42be-9e73-999f08f86b92
      INFRAHUB_STORAGE_DRIVER: s3
      AWS_ACCESS_KEY_ID: xxxx
      AWS_SECRET_ACCESS_KEY: xxxx
      AWS_S3_BUCKET_NAME: infrahub-data
      AWS_S3_ENDPOINT_URL: https://s3

infrahubTaskWorker:
  replicas: 3
  affinity:
    podAntiAffinity:
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchExpressions:
            - key: service
              operator: In
              values:
              - infrahub-task-worker
          topologyKey: topology.kubernetes.io/zone

neo4j:
  services:
    admin:
      enabled: true
  volumes:
    data:
      mode: dynamic
      dynamic:
        storageClassName: premium-rwo
        requests:
          storage: 100Gi
```

:::warning

Be sure to replace the placeholder values with your actual values.

:::

#### Step 2: install the chart

Install using a local chart:

```bash
helm install infrahub -f values.yml path/to/infrahub/chart
```

Or install using the OpsMill registry:

```bash
helm install infrahub -f values.yml oci://registry.opsmill.io/opsmill/chart/infrahub
```

:::success

Verify the installation by checking that all pods are running:

```bash
kubectl get pods -l app=infrahub
```

:::

</TabItem>
</Tabs>

## Enterprise<EnterpriseBadge />

Infrahub Enterprise is based on the Community version, with several enhancements for:

- Enterprise features
- High availability
- Better performance
- Security hardening (Docker image, etc.)

Infrahub Enterprise can be deployed using the same methods as Infrahub Community.

<Tabs>
<TabItem value="Docker compose via curl" default>

### Using curl and Docker Compose

To quickly spin up the latest Infrahub Enterprise locally, retrieve the Docker Compose file from [infrahub.opsmill.io/enterprise](https://infrahub.opsmill.io/enterprise).

You can also specify a specific version in the URL:

- [https://infrahub.opsmill.io/enterprise/1.3.5](https://infrahub.opsmill.io/enterprise/1.3.5)

You can also specify a sizing preset in the URL. This will automatically configure replica count for each component according to your sizing plan:

- [https://infrahub.opsmill.io/enterprise?size=small](https://infrahub.opsmill.io/enterprise?size=small) (requires 16 GB of RAM)
- [https://infrahub.opsmill.io/enterprise/1.3.5?size=medium](https://infrahub.opsmill.io/enterprise/1.3.5?size=medium) (requires 32 GB of RAM)
- [https://infrahub.opsmill.io/enterprise/stable?size=large](https://infrahub.opsmill.io/enterprise/stable?size=large) (requires 64 GB of RAM)

| Size   | Total required memory | API workers | Task workers | Task manager API workers | Task manager background workers | DB heap size | DB page cache size |
| ----------- | ----- | - | - | - | - | --- | --- |
| small       | 16 GB  | 4 | 2 | 1 | 1 | 8G  | 1G  |
| medium      | 32 GB | 4 | 4 | 2 | 2 | 24G | 4G  |
| medium-data | 32 GB | 4 | 2 | 1 | 1 | 24G | 4G  |
| large       | 64 GB | 4 | 8 | 4 | 2 | 31G | 16G |
| large-data  | 64 GB | 4 | 2 | 1 | 1 | 31G | 16G |

#### Prerequisites

- [Docker](https://docs.docker.com/engine/install/) (version 24.x minimum)
- [Docker Compose](https://docs.docker.com/compose/install/)

#### Start an Infrahub Enterprise environment

<Tabs groupId="os">
<TabItem value="macOS" default>

```bash
curl https://infrahub.opsmill.io/enterprise > docker-compose.yml
docker compose -p infrahub up -d
```

</TabItem>
<TabItem value="Linux">

```bash
curl https://infrahub.opsmill.io/enterprise > docker-compose.yml
sudo docker compose -p infrahub up -d
```

</TabItem>
</Tabs>

#### Stop and remove an Infrahub Enterprise environment

<Tabs groupId="os">
<TabItem value="macOS" default>

```bash
curl https://infrahub.opsmill.io/enterprise > docker-compose.yml
docker compose -p infrahub down -v
```

</TabItem>
<TabItem value="Linux">

```bash
curl https://infrahub.opsmill.io/enterprise > docker-compose.yml
sudo docker compose -p infrahub down -v
```

</TabItem>
</Tabs>

</TabItem>
<TabItem value="Kubernetes with Helm">

### Using Helm and Kubernetes

The Enterprise Helm chart is based on the original Infrahub chart and uses it as a Helm dependency.
Most configuration related to Infrahub goes inside the `infrahub` top-level key.

<ReferenceLink title="Infrahub Enterprise Helm Chart" url="https://github.com/opsmill/infrahub-helm/tree/stable/charts/infrahub-enterprise" openInNewTab />
<ReferenceLink title="ArtifactHub" url="https://artifacthub.io/packages/helm/infrahub-enterprise/infrahub-enterprise" openInNewTab />

#### Production deployment requirements

The following are required for production deployments using Helm:

- Data persistence for the database must be enabled
- Multiple replicas of the Infrahub API Server and Infrahub Task workers should be deployed: you can use the `affinity` variable to define the affinity policy for the pods
- S3 storage should be configured for the Infrahub API Server, it is required if you have multiple replicas

:::warning

We do not recommend using the included dependencies (Neo4j, RabbitMQ, Redis) for production.
They are present to ease deployment on non-production environments.

:::

#### Prerequisites

- A Kubernetes cluster
- [Helm](https://helm.sh/docs/intro/install/) installed on your system

#### Step 1: Fill in the values file

Create a `values.yml` file with the following configuration:

```yaml
infrahub:
  infrahubServer:
    replicas: 3
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: service
                operator: In
                values:
                - infrahub-server
            topologyKey: topology.kubernetes.io/zone
    persistence:
      enabled: false
    ingress:
      enabled: true
    infrahubServer:
      env:
        INFRAHUB_ALLOW_ANONYMOUS_ACCESS: "true"
        INFRAHUB_CACHE_PORT: 6379
        INFRAHUB_DB_TYPE: neo4j
        INFRAHUB_LOG_LEVEL: INFO
        INFRAHUB_PRODUCTION: "true"
        INFRAHUB_INITIAL_ADMIN_TOKEN: 06438eb2-8019-4776-878c-0941b1f1d1ec
        INFRAHUB_SECURITY_SECRET_KEY: 327f747f-efac-42be-9e73-999f08f86b92
        INFRAHUB_STORAGE_DRIVER: s3
        AWS_ACCESS_KEY_ID: xxxx
        AWS_SECRET_ACCESS_KEY: xxxx
        AWS_S3_BUCKET_NAME: infrahub-data
        AWS_S3_ENDPOINT_URL: https://s3

  infrahubTaskWorker:
    replicas: 3
    affinity:
      podAntiAffinity:
        requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchExpressions:
              - key: service
                operator: In
                values:
                - infrahub-task-worker
            topologyKey: topology.kubernetes.io/zone

  neo4j:
    services:
      admin:
        enabled: true
    volumes:
      data:
        mode: dynamic
        dynamic:
          storageClassName: premium-rwo
          requests:
            storage: 100Gi
```

:::warning
Be sure to replace the placeholder values with your actual values.
:::

You can also use the configuration preset values.yaml file. This will automatically configure replica count for each component according to your sizing plan.
They are available here:

<ReferenceLink title="Small size config preset (requires 16 GB of RAM)" url="https://github.com/opsmill/infrahub-helm/blob/stable/charts/infrahub-enterprise/values.small.yaml" openInNewTab />
<ReferenceLink title="Medium size config preset (requires 32 GB of RAM)" url="https://github.com/opsmill/infrahub-helm/blob/stable/charts/infrahub-enterprise/values.medium.yaml" openInNewTab />
<ReferenceLink title="Medium (data) size config preset (requires 32 GB of RAM)" url="https://github.com/opsmill/infrahub-helm/blob/stable/charts/infrahub-enterprise/values.medium-data.yaml" openInNewTab />
<ReferenceLink title="Large size config preset (requires 64 GB of RAM)" url="https://github.com/opsmill/infrahub-helm/blob/stable/charts/infrahub-enterprise/values.large.yaml" openInNewTab />
<ReferenceLink title="Large (data) size config preset (requires 64 GB of RAM)" url="https://github.com/opsmill/infrahub-helm/blob/stable/charts/infrahub-enterprise/values.large-data.yaml" openInNewTab />

#### Step 2: install the chart

Install using a local chart:

```bash
helm install infrahub -f values.yml path/to/infrahub-enterprise/chart
```

Or install using the OpsMill registry:

```bash
helm install infrahub -f values.yml oci://registry.opsmill.io/opsmill/chart/infrahub-enterprise
```

:::success

Verify the installation by checking that all pods are running:

```bash
kubectl get pods -l app=infrahub
```

:::

</TabItem>
</Tabs>

## High availability deployment examples

The following examples demonstrate how to deploy Infrahub in a highly available configuration using Kubernetes. These deployments provide resilience, scalability, and are suitable for production environments.

:::warning

These examples are for reference purposes and may require customization for your specific environment. Ensure you understand the requirements and dependencies before deploying to production.

:::

### Using Terraform

<details>
  <summary>Example HA deployment using Terraform on a 3-node Kubernetes cluster</summary>
  <CodeBlock language="hcl" title="deploy.tf">{import_deployTf}</CodeBlock>
</details>

### Using Helm

<details>
  <summary>Example HA deployment using Helm and kubectl</summary>

  <CodeBlock language="yaml" title="redis-values.yaml">{import_redisValuesYaml}</CodeBlock>

  ```bash
  helm repo add bitnami https://charts.bitnami.com/bitnami
  helm repo update
  helm install cache bitnami/redis --version 19.5.2 --namespace infrahub --create-namespace --values redis-values.yaml
  ```

  <CodeBlock language="yaml" title="redis-sentinel-proxy-deploy.yaml">{import_redisSentinelDeployYaml}</CodeBlock>

  <CodeBlock language="yaml" title="redis-sentinel-proxy-svc.yaml">{import_redisSentinelSvcYaml}</CodeBlock>

  ```bash
  kubectl apply -f redis-sentinel-proxy-deploy.yaml --namespace infrahub
  kubectl apply -f redis-sentinel-proxy-svc.yaml --namespace infrahub
  ```

  <CodeBlock language="yaml" title="rabbitmq-values.yaml">{import_rabbitmqValuesYaml}</CodeBlock>

  ```bash
  helm install messagequeue oci://registry-1.docker.io/bitnamicharts/rabbitmq --version 14.4.1 --namespace infrahub --create-namespace --values rabbitmq-values.yaml
  ```

  <CodeBlock language="yaml" title="neo4j-values.yaml">{import_neo4jValuesYaml}</CodeBlock>

  ```bash
  helm repo add neo4j https://helm.neo4j.com/neo4j/
  helm repo update
  kubectl create secret generic neo4j-user --namespace infrahub --from-literal=NEO4J_AUTH=neo4j/admin
  helm install database-0 neo4j/neo4j --version 2025.03.0 --namespace infrahub --values neo4j-values.yaml
  helm install database-1 neo4j/neo4j --version 2025.03.0 --namespace infrahub --values neo4j-values.yaml
  helm install database-2 neo4j/neo4j --version 2025.03.0 --namespace infrahub --values neo4j-values.yaml
  ```

  Once all instances are running, install the headless service:

  <CodeBlock language="yaml" title="service-values.yaml">{import_serviceValuesYaml}</CodeBlock>

  ```bash
  helm install database-service neo4j/neo4j-headless-service --version 2025.03.0 --namespace infrahub --values service-values.yaml
  ```

  <CodeBlock language="yaml" title="minio-values.yaml">{import_minioValuesYaml}</CodeBlock>

  ```bash
  helm install objectstore oci://registry-1.docker.io/bitnamicharts/minio --version 15.0.5 --namespace infrahub --create-namespace --values minio-values.yaml
  ```

  ```bash
  helm repo add cnpg https://cloudnative-pg.github.io/charts
  helm repo update
  helm install cnpg cnpg/cloudnative-pg --version 0.23.2 --namespace infrahub --create-namespace
  kubectl create secret generic prefect-user --namespace infrahub --from-literal=username=prefect --from-literal=password=prefect
  ```

  <CodeBlock language="yaml" title="taskmanagerdb.yaml">{import_taskmanagerdbYaml}</CodeBlock>

  ```bash
  kubectl apply -f taskmanagerdb.yaml --namespace infrahub
  ```

  <CodeBlock language="yaml" title="prefect-values.yaml">{import_prefectValuesYaml}</CodeBlock>

  ```bash
  helm install taskmanager prefect-server --repo https://prefecthq.github.io/prefect-helm --version 2025.7.31204438 --namespace infrahub --create-namespace --values prefect-values.yaml
  ```

  <CodeBlock language="yaml" title="infrahub-values.yaml">{import_infrahubValuesYaml}</CodeBlock>

  ```bash
  helm install infrahub oci://registry.opsmill.io/opsmill/chart/infrahub-enterprise --version 3.9.4 --namespace infrahub --create-namespace --values infrahub-values.yaml
  ```

</details>

### Load balancer configuration

For high availability deployments, a load balancer is useful to distribute traffic across multiple Infrahub API server instances. The load balancer must be configured to properly handle both HTTP/HTTPS and WebSocket connections.

<details>
  <summary>HAProxy configuration example</summary>

```haproxy
global
    maxconn 4096
    log stdout local0

defaults
    mode http
    timeout connect 5000ms
    timeout client 50000ms
    timeout server 50000ms
    option httplog

# Frontend for Infrahub API
frontend infrahub_frontend
    bind *:80
    bind *:443 ssl crt /etc/ssl/certs/infrahub.pem

    # Redirect HTTP to HTTPS
    redirect scheme https if !{ ssl_fc }

    # ACL for WebSocket connections
    acl is_websocket hdr(Upgrade) -i WebSocket
    acl is_websocket_path path_beg /ws /graphql-ws

    # Use WebSocket backend for WS connections
    use_backend infrahub_websocket if is_websocket || is_websocket_path

    # Default to HTTP backend
    default_backend infrahub_http

# Backend for regular HTTP/HTTPS traffic
backend infrahub_http
    balance roundrobin
    option httpchk GET /api/health
    http-check expect status 200

    # Sticky sessions for GraphQL subscriptions
    cookie SERVERID insert indirect nocache

    server infrahub-api-1 10.0.1.10:8000 check cookie api1
    server infrahub-api-2 10.0.1.11:8000 check cookie api2
    server infrahub-api-3 10.0.1.12:8000 check cookie api3

# Backend for WebSocket connections
backend infrahub_websocket
    balance source
    option http-server-close
    option forceclose

    # WebSocket specific health check
    option httpchk GET /api/health
    http-check expect status 200

    server infrahub-api-1 10.0.1.10:8000 check
    server infrahub-api-2 10.0.1.11:8000 check
    server infrahub-api-3 10.0.1.12:8000 check
```

</details>

<details>
  <summary>NGINX configuration example</summary>

```nginx
upstream infrahub_backend {
    # IP hash for session affinity
    ip_hash;

    server 10.0.1.10:8000 max_fails=3 fail_timeout=30s;
    server 10.0.1.11:8000 max_fails=3 fail_timeout=30s;
    server 10.0.1.12:8000 max_fails=3 fail_timeout=30s;
}

server {
    listen 80;
    server_name infrahub.example.com;

    # Redirect HTTP to HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    server_name infrahub.example.com;

    ssl_certificate /etc/ssl/certs/infrahub.pem;
    ssl_certificate_key /etc/ssl/private/infrahub.key;
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers HIGH:!aNULL:!MD5;

    # Client body size for large GraphQL queries
    client_max_body_size 10M;

    # Timeouts for long-running operations
    proxy_connect_timeout 600;
    proxy_send_timeout 600;
    proxy_read_timeout 600;
    send_timeout 600;

    # Health check endpoint
    location /health {
        access_log off;
        proxy_pass http://infrahub_backend/api/health;
    }

    # WebSocket support for GraphQL subscriptions
    location ~ ^/(ws|graphql-ws) {
        proxy_pass http://infrahub_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Disable buffering for WebSocket
        proxy_buffering off;
    }

    # Regular API traffic
    location / {
        proxy_pass http://infrahub_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Enable keepalive
        proxy_http_version 1.1;
        proxy_set_header Connection "";
    }
}
```

</details>

## Related resources

- [Database backup and restore](./database-backup.mdx) - Learn how to backup and restore your Infrahub database
- [High availability architecture](../topics/architecture.mdx#scalability-and-high-availability) - Understand resilient deployment architectures
- [Local demo environment](../topics/local-demo-environment.mdx) - Explore the demo environment configuration
- [Hardware requirements](../topics/hardware-requirements.mdx) - Review system requirements
